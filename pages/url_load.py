from io import StringIO

import streamlit as st
from langchain.callbacks.streamlit import StreamlitCallbackHandler
from langchain_core.callbacks import BaseCallbackHandler
from langchain_core.messages import ChatMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langsmith import traceable

from app.graph.learning_graph import LearningGraph
from app.tools.loader.file_loader import FileLoader
from loguru import logger

from app.tools.models.model_tokens import models_tokens
from app.tools.models.openai_embeder import OpenaiEmbeder
from app.tools.models.select_llms import get_llms
from app.tools.storage.paradeDB import ParadeDB
from dotenv import load_dotenv
import os

# åŠ è½½ .env æ–‡ä»¶
load_dotenv()

langchain_tracing_v2 = os.getenv('LANGCHAIN_TRACING_V2')
langchain_api_key = os.getenv('LANGCHAIN_API_KEY')


st.set_page_config(
    page_title="æˆ‘çš„çŸ¥è¯†åº“", page_icon="ğŸ“š", layout="wide"
)


def scrape_url(url):
    learning_graph = LearningGraph(
        config={
            "llm": {
                "model": "gpt-3.5-turbo-ca",
            },
        },
        source=url,
    )
    result = learning_graph.run()
    return result


class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str | dict, **kwargs) -> None:
        tokens = token if isinstance(token, str) else token["content"]
        self.text += tokens
        self.container.markdown(self.text)


def scrape_file(file):
    learning_graph = LearningGraph(
        config={
            "llm": {
                "model": "gpt-3.5-turbo-ca",
            },
        },
        source="file",
    )
    result = learning_graph.run()
    return result


all_provider = set(models_tokens.keys())

with st.sidebar:
    url = st.sidebar.text_area(
        "è¯·è¾“å…¥url"
    )
    url_list = url.split("\n")
    sent = st.button(
        "è½½å…¥:magic_wand:",
    )
    st.sidebar.markdown("-----------")
    st.sidebar.success("é€‰æ‹©ä¸€ä¸ªæ¨¡å‹.")
    provider = st.sidebar.selectbox(
        "é€‰æ‹©ä¸€ä¸ªæ¨¡å‹æä¾›å•†",
        all_provider
    )
    model_name = st.sidebar.selectbox(
        "é€‰æ‹©ä¸€ä¸ªæ¨¡å‹",
        models_tokens[provider]
    )
    system_prompt = st.sidebar.text_area(
        "ç³»ç»Ÿæç¤º",
        "ä½ æ˜¯ä¸€ä¸ªèƒ½å¤Ÿæ£€ç´¢çŸ¥è¯†å¹¶æ ¹æ®æ£€ç´¢åˆ°çŸ¥è¯†å›ç­”ç”¨æˆ·é—®é¢˜çš„æœºå™¨äºº"
    )
    set_system_prompt = st.button('é‡ç½®å¯¹è¯ğŸ”')

    if sent:
        for url in url_list:
            res = scrape_url(url)
            if res != "çˆ¬å–å¤±è´¥":
                st.success("çˆ¬å–æˆåŠŸ")
            else:
                st.error("çˆ¬å–å¤±è´¥")
                logger.error(f"{url} çˆ¬å–å¤±è´¥")

st.title("ğŸ” å’ŒçŸ¥è¯†åº“èŠå¤©")

# @traceable
def run_chat():
    if set_system_prompt:
        st.session_state["messages"] = [ChatMessage(role="system", content=system_prompt)]

    if "messages" not in st.session_state:
        st.session_state["messages"] = [ChatMessage(role="system", content=system_prompt)]

    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

    if prompt := st.chat_input():
        st.session_state.messages.append(ChatMessage(role="user", content=prompt))
        st.chat_message("user").write(prompt)

        stream_handler = StreamHandler(st.empty())
        llm_config = {
            "provider": provider,
            "model": model_name,
            "streaming": True,
            "callbacks": [stream_handler],
        }
        llm = get_llms(llm_config)

        template = """ä½¿ç”¨ä¸‹é¢çš„æ£€ç´¢åˆ°çš„æ–‡ç« å›ç­”æˆ‘çš„é—®é¢˜

        {context}
    
        Question: {question}
    
        ä½ çš„å›ç­”:"""
        custom_rag_prompt = PromptTemplate.from_template(template)

        db = ParadeDB(
            connection='postgresql+psycopg://pgvector:pgvector@localhost:5432/ai_dev',
            embedding_length=1536,
            embedding_function=OpenaiEmbeder()
        )
        retriever = db.as_retriever(
            search_kwargs={"k": 4}
        )

        def format_docs(docs):
            return "\n\n".join(doc.page_content for doc in docs)

        rag_chain = (
                {"context": retriever | format_docs, "question": RunnablePassthrough()}
                | custom_rag_prompt
                | llm
                | StrOutputParser()
        )

        with st.chat_message("assistant"):
            st_cb = StreamlitCallbackHandler(st.container(), expand_new_thoughts=False)
            response = rag_chain.invoke(prompt,{"callbacks": [st_cb]})
            st.session_state.messages.append(ChatMessage(role="assistant", content=response))
            # st.write(response)

run_chat()